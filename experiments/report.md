# INF-3600 GenAI - freil3970

## 1. Introduction

This report presents three controlled experiments that explore how Transformer architecture choices affect the quality of a small language model. We varied model depth, embedding width, and number of attention heads one at a time, training 12 models in total on the TinyStories dataset.

TinyStories is a synthetic dataset of short children's stories generated by GPT-4. It uses simple vocabulary and sentence structures, but still contains enough variety in narrative patterns to reveal meaningful differences between model configurations. We used character-level tokenization (vocab_size = 228), which means the models must learn to form words from individual characters before they can learn any higher-level structure.

All models were trained using nanoGPT on an Apple M-series GPU (MPS backend) for 5000 iterations each. We used `dropout = 0.0` as recommended, with a cosine learning rate schedule from 1e-3 down to 1e-4 and 100 warmup steps. Each model generated 5 samples of 1000 tokens for qualitative evaluation.

## 2. Methods

We ran three sweeps, each varying a single architectural dimension while holding the others fixed:

| | Experiment 1 (Depth) | Experiment 2 (Width) | Experiment 3 (Heads) |
|---|---|---|---|
| **Varied** | `n_layer` ∈ {2, 4, 8, 12} | `n_embd` ∈ {128, 192, 256, 384} | `n_head` ∈ {4, 8, 16, 32} |
| **n_layer** | varied | 6 | 6 |
| **n_embd** | 256 | varied | 256 |
| **n_head** | 8 | 8 | varied |
| **dropout** | 0.0 | 0.0 | 0.0 |
| **block_size** | 256 | 256 | 256 |
| **batch_size** | 64 | 64 | 64 |
| **max_iters** | 5000 | 5000 | 5000 |

Note that `n_embd` must be divisible by `n_head`. In experiment 3 this gives per-head dimensions of 64, 32, 16, and 8 respectively for the four configurations.

## 3. Results

### 3.1 Parameter Counts

| Config | Parameters |
|--------|-----------|
| **Exp 1:** n_layer=2 | 1.70M |
| n_layer=4 | 3.27M |
| n_layer=8 | 6.42M |
| n_layer=12 | 9.57M |
| **Exp 2:** n_embd=128 | 1.24M |
| n_embd=192 | 2.75M |
| n_embd=256 | 4.85M |
| n_embd=384 | 10.81M |
| **Exp 3:** n_head=4 | 4.85M |
| n_head=8 | 4.85M |
| n_head=16 | 4.85M |
| n_head=32 | 4.85M |

Parameters scale linearly with depth (~1.57M per layer) and roughly quadratically with width (the MLP layers contain matrices of size n_embd × 4·n_embd). Changing the number of attention heads does not change the parameter count at all — the Q, K, V projection matrices stay the same size regardless of how they are split across heads.

### 3.2 Validation Loss

| Step | n_layer=2 | =4 | =8 | =12 | n_embd=128 | =192 | =256 | =384 | n_head=4 | =8 | =16 | =32 |
|------|-----------|-----|------|------|------------|------|------|------|----------|-----|------|------|
| 250 | 1.8388 | 1.6812 | 1.6527 | 1.6811 | 1.9282 | 1.7378 | 1.7103 | 1.5942 | 1.6841 | 1.7103 | 1.7243 | 1.7116 |
| 1000 | 0.9693 | 0.8857 | 0.8407 | 0.8348 | 1.0461 | 0.9237 | 0.8534 | 0.7747 | 0.8484 | 0.8494 | 0.8576 | 0.8607 |
| 2500 | 0.7783 | 0.7010 | 0.6638 | 0.6509 | 0.7924 | 0.7151 | 0.6836 | 0.6343 | 0.6788 | 0.6836 | 0.6847 | 0.6923 |
| 5000 | 0.6920 | 0.6193 | 0.5852 | 0.5703 | 0.6968 | 0.6324 | 0.5996 | 0.5567 | 0.5954 | 0.6000 | 0.6011 | 0.6062 |

A few things stand out. In experiments 1 and 2, more capacity consistently leads to lower loss, but with diminishing returns — the gap between the two largest configs is always smaller than between the two smallest. In experiment 3, the differences are tiny (total spread of just 0.011 compared to 0.122 and 0.140 in experiments 1 and 2).

None of the models showed signs of overfitting during training. Train and validation losses stayed closely aligned throughout, which makes sense given the large dataset relative to model size, and the lack of dropout.

### 3.3 Training Speed

| Config | ms/iter |
|--------|---------|
| **Exp 1:** n_layer=2 | ~88 |
| n_layer=12 | ~530 |
| **Exp 2:** n_embd=128 | ~153 |
| n_embd=384 | ~435 |
| **Exp 3:** n_head=4 | ~207 |
| n_head=32 | ~640 |

Training time scales roughly linearly with depth and width, as expected. The attention head result is more surprising — n_head=32 is about 3x slower than n_head=4 despite having identical parameter counts. On the MPS backend, computing many small attention matrices appears to be less efficient than fewer large ones.

### 3.4 Vocabulary Diversity

We measured diversity as unique_words / total_words across all 5 generated samples per model.

| Config | Unique | Total | Ratio |
|--------|--------|-------|-------|
| **Exp 1:** n_layer=2 | 322 | 1031 | 0.312 |
| n_layer=4 | 266 | 1006 | 0.264 |
| n_layer=8 | 262 | 959 | 0.273 |
| n_layer=12 | 271 | 909 | 0.298 |
| **Exp 2:** n_embd=128 | 254 | 848 | 0.300 |
| n_embd=192 | 264 | 879 | 0.300 |
| n_embd=256 | 271 | 882 | 0.307 |
| n_embd=384 | 277 | 925 | 0.299 |
| **Exp 3:** n_head=4 | 181 | 576 | 0.314 |
| n_head=8 | 179 | 571 | 0.313 |
| n_head=16 | 175 | 537 | 0.326 |
| n_head=32 | 188 | 606 | 0.310 |

Diversity ratios are fairly stable across all configurations (roughly 0.26–0.33). The n_layer=2 model has the highest diversity in experiment 1, which initially seems counterintuitive. But reading the actual samples makes it clear: the shallow model uses many different words because it can't maintain coherent topics — it jumps between unrelated ideas, introducing new words each time. The deeper models stick to fewer words because they've learned to stay on topic within a story.

### 3.5 Qualitative Analysis

#### Experiment 1 — Depth

The clearest quality progression is in this experiment. With 2 layers the model barely forms coherent sentences:

> **n_layer=2:** *"Hey, stop stand can't. That was important," Tom said. Sue ran away, and she got his nose.* [...] *"Wow, that is so much!" Ben says. "Wow, look at the candy!" says.*

Grammar is broken, characters appear and vanish, and dialogue attribution falls apart. By 4 layers, the model has picked up the basic TinyStories template (character + problem + resolution) but still loses track of what things are:

> **n_layer=4:** *He asked his mom, "Can I play with the surprise?" His bear said, "Yes, let's climb the surprise together!" They found the surprise together and ate it all of it.*

At 8 layers, stories have a recognizable arc with characters that mostly persist:

> **n_layer=8:** *One day, Joy went to the store with her mom. She wanted to build a big store. She looked at the store and smiled. Then she saw a tiny man with a big box.*

And at 12 layers, the output reads most naturally:

> **n_layer=12:** *At the park, Tim met a new friend named Sam. Sam said, "Tim, I like your rock too. Can I have a surprise, please?" Tim said, "Yes, you can have it." Tim and Sam played on the sun and had lots of fun.*

Characters are introduced properly, dialogue flows between them, and the story follows a logical sequence. There are still oddities ("played on the sun"), but the structure is solid.

#### Experiment 2 — Width

The width sweep shows a similar pattern, but more about language fluency than narrative structure. The smallest model (n_embd=128) produces borderline gibberish:

> **n_embd=128:** *Tom and Sam are twins. They kind, they see a cabinet on the food. The cabinets is very big and sees they read they understood. Tom and Sam set on the cabinet and Tom and Ben. They drop the caber it and smooter.*

Words like "smooter" don't exist — the model can't reliably form valid words at this embedding size. By n_embd=384, the text is fluent and grammatically mostly correct:

> **n_embd=384:** *One day, the fair wanted to be friends. So, the frog jumped out of the house and started to bark. The fair did not split in the play.*

The wider model writes more confidently but also more formulaically. It has learned the TinyStories templates well and tends to reproduce them faithfully. Whether this counts as "over-ambitious" prediction is debatable — the model isn't hallucinating complex scenarios beyond its training data, but it is sticking very closely to learned patterns rather than producing varied output. The diversity metric (0.299 vs 0.300 for the smallest model) confirms that wider models don't actually produce more diverse text — they just produce more fluent versions of the same kinds of stories.

#### Experiment 3 — Heads

The qualitative differences are minimal, which matches the loss numbers. Both extremes produce similar-quality text:

> **n_head=4:** *Once upon a time, there was a little kids named Tim. Tim liked to play outside and set off. He was very happy and excited.*

> **n_head=32:** *One day, a restless dog named Spot wanted to help. He wanted to go upstairs. He wanted to walk to the park with his friends.*

Both maintain basic coherence and narrative structure. The n_head=32 output occasionally shows slightly more repetitive sentence openings ("He wanted to..."), but the differences are subtle enough that they could be sampling noise.

## 4. Discussion

Across all three experiments, the clearest finding is that **model capacity matters more than model structure** at this scale. Both depth and width directly increase the number of parameters, and both lead to clear improvements in validation loss and output quality. Width has the largest effect per experiment (loss spread of 0.140), followed by depth (0.122). The number of attention heads, which changes structure without changing capacity, has almost no effect (0.011).

**Diminishing returns** are visible in both capacity experiments. In the depth sweep, going from 2 to 4 layers gives a loss improvement of 0.073 while adding 1.57M parameters, but going from 8 to 12 layers only gives 0.015 despite adding twice as many (3.15M). TinyStories has simple enough structure that 6-8 layers seem to capture most of the learnable patterns. Similarly in the width sweep, the jump from 128 to 192 (0.064) is proportionally larger than from 256 to 384 (0.043), even though the latter adds far more parameters.

An interesting observation is that **the shallowest and narrowest models actually produce more diverse text** (by the unique/total words metric), but this diversity comes from incoherence rather than creativity. They jump between topics because they haven't learned to sustain a narrative, introducing new words with each jump. The better models are more repetitive because they've learned what a TinyStories story looks like and stick to that template.

We did not observe traditional overfitting (validation loss diverging from training loss) in any configuration. This is likely because the TinyStories dataset is large relative to even our biggest model (10.81M parameters). With dropout set to 0.0, overfitting would probably appear with either longer training or smaller datasets.

The attention head result deserves special comment. Conventional wisdom from large-scale models suggests that more attention heads allow the model to attend to different aspects of the input simultaneously. But at this scale, with n_embd=256, using 32 heads means each head only operates in an 8-dimensional space — likely too small to represent useful attention patterns. Fewer heads with larger per-head dimensions (n_head=4 gives 64 dimensions per head) performed slightly better and trained 3x faster.

## 5. Conclusion

The most impactful architectural choice for small Transformer models is embedding width, followed by depth. Both increase model capacity, and both show clear improvements in both loss and text quality up to a point of diminishing returns. The number of attention heads, at least at this scale, has negligible impact on model quality but a large impact on training speed — fewer heads are preferable.

If we were to continue this investigation, we would explore the effect of dropout (especially for the larger models where overfitting might eventually appear with longer training), and try to find the point where adding more capacity stops helping altogether. Training for more iterations would also reveal whether the models are still improving at step 5000 or have effectively converged — our loss curves suggest most models are still slowly improving, so longer training could shift some of these results.
